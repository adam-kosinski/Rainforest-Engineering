{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline v3 adds using the CLIP model to classify segmentation masks to verify that they are of a plant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0],\n",
    "                  sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.6]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green',\n",
    "               marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red',\n",
    "               marker='*', s=marker_size, edgecolor='white', linewidth=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duker\\anaconda3\\envs\\plant_id\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\duker\\anaconda3\\envs\\plant_id\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\duker\\anaconda3\\envs\\plant_id\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\duker\\anaconda3\\envs\\plant_id\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Users\\duker\\anaconda3\\envs\\plant_id\\lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileSAM Loaded\n",
      "CLIP loaded\n",
      "ClipSeg loaded\n",
      "DepthAnything Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation, CLIPSegProcessor, CLIPSegForImageSegmentation, CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# set up segmentation model\n",
    "\n",
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"../weights/mobile_sam.pt\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mobile_sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "mobile_sam.to(device=device)\n",
    "mobile_sam.eval()\n",
    "print(\"MobileSAM Loaded\")\n",
    "\n",
    "\n",
    "# set up clip model\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "print(\"CLIP loaded\")\n",
    "\n",
    "\n",
    "# set up clipseg model\n",
    "clipseg_processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "clipseg_model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "print(\"ClipSeg loaded\")\n",
    "\n",
    "\n",
    "# set up depth model\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "print(\"DepthAnything Loaded\")\n",
    "\n",
    "def get_inverse_depth(np_image):\n",
    "    # prepare image for the model\n",
    "    image = Image.fromarray(np_image)\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # normalize and return\n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "    output = output / np.max(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_image(path):\n",
    "    with Image.open(path).convert(\"RGB\") as img:\n",
    "        full_res_image = np.array(img)\n",
    "\n",
    "    # resize so not too big for GPU\n",
    "    max_width = 1024\n",
    "    height, width = full_res_image.shape[:2]\n",
    "    new_width = max_width if width > max_width else width\n",
    "    image_shrink_factor = new_width / width  # will be used later for sampling from full res image\n",
    "    new_height = int(height * image_shrink_factor)\n",
    "    image = cv2.resize(full_res_image, (new_width, new_height))\n",
    "\n",
    "    return image, full_res_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower/Object Search with ClipSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def find_objects(np_image, prompts, target_prompt=\"flower\", display_results=True, logit_threshold=0.25):\n",
    "    # threshold between 0 and 1, closer to 0 means more areas will be selected\n",
    "    assert target_prompt in prompts, \"target prompt must be one of the prompts\"\n",
    "\n",
    "    # image is a numpy image, convert to PIL\n",
    "    image = Image.fromarray(np_image)\n",
    "    \n",
    "    # prepare clipseg model inputs\n",
    "    logging.disable(logging.WARNING)  # get rid of annoying padding message\n",
    "    inputs = clipseg_processor(text=prompts, images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\")\n",
    "    logging.disable(logging.NOTSET)\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        outputs = clipseg_model(**inputs)\n",
    "        preds = outputs.logits.unsqueeze(1)\n",
    "\n",
    "    # get resized logits heatmap for target prompt and find bounding boxes\n",
    "    target_prompt_idx = prompts.index(target_prompt)\n",
    "    target_logits_square = torch.sigmoid(preds[target_prompt_idx][0]).numpy()\n",
    "    target_logits = cv2.resize(target_logits_square, (image.width, image.height))\n",
    "    \n",
    "    bboxs = get_bounding_boxes(target_logits, logit_threshold)\n",
    "\n",
    "    # display\n",
    "    if display_results:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Found objects for prompt: {target_prompt}\")\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(target_logits, vmin=0, vmax=1, alpha=0.5)\n",
    "        for bbox in bboxs:\n",
    "            x,y,w,h = bbox\n",
    "            plt.gca().add_patch(plt.Rectangle((x, y), w, h, edgecolor='red', facecolor=(0,0,0,0), lw=1))\n",
    "    \n",
    "    return bboxs\n",
    "\n",
    "\n",
    "def get_bounding_boxes(heatmap, logit_threshold=0.4):\n",
    "    # heatmap should range from 0 to 1\n",
    "    assert np.min(heatmap) >= 0 and np.max(heatmap) <= 1, \"Heatmap values should range from 0 to 1\"\n",
    "\n",
    "    # based on this method: https://stackoverflow.com/questions/58419893/generating-bounding-boxes-from-heatmap-data\n",
    "\n",
    "    # blur the sigmoid logits for more reliable thresholding\n",
    "    blur = cv2.GaussianBlur(heatmap, (51,51), 0)\n",
    "\n",
    "    # threshold\n",
    "    thresholded = cv2.threshold((255*blur).astype(\"uint8\"), 255*logit_threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "    # thresholded = cv2.threshold((255*sigmoid_logits).astype(\"uint8\"), 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # get contours / bounding boxes\n",
    "    bboxs = []\n",
    "    contours = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "    for c in contours:\n",
    "        bboxs.append(cv2.boundingRect(c)) # x,y,w,h\n",
    "    \n",
    "    return bboxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Generation and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(np_image):\n",
    "  mask_generator = SamAutomaticMaskGenerator(mobile_sam, stability_score_thresh=0.85, points_per_side=16)\n",
    "  unfiltered_mask_data = mask_generator.generate(np_image)\n",
    "\n",
    "  # filter submasks and masks that can be broken up\n",
    "  mask_data = filter_masks(unfiltered_mask_data)\n",
    "\n",
    "  return mask_data, unfiltered_mask_data\n",
    "\n",
    "\n",
    "\n",
    "def is_made_of_submasks(mask_data, k):\n",
    "  \"\"\"Tests if the kth mask is mostly covered by the union of submasks that are a significant proportion of this mask.\n",
    "  If this is the case, it's probably better to use the submasks instead of this mask.\n",
    "  \"\"\"\n",
    "\n",
    "  min_intersection_fraction = 0.1\n",
    "  submask_union = np.full(mask_data[k]['segmentation'].shape, False)\n",
    "\n",
    "  for i, mask_object in enumerate(mask_data):\n",
    "    if i == k:\n",
    "      continue\n",
    "\n",
    "    # ignore bigger masks, probably not a submask\n",
    "    if mask_object['area'] > mask_data[k]['area']:\n",
    "      continue\n",
    "\n",
    "    # ignore tiny masks\n",
    "    if mask_object['area'] < min_intersection_fraction * mask_data[k]['area']:\n",
    "      continue\n",
    "\n",
    "    intersection = mask_object['segmentation'] & mask_data[k]['segmentation']\n",
    "    if np.count_nonzero(intersection) / mask_data[k]['area'] > min_intersection_fraction:\n",
    "      submask_union = submask_union | intersection\n",
    "\n",
    "    # if submasks managed to cover an appreciable fraction of this mask, return true\n",
    "    # from testing, \"appreciable fraction\" should actually be quite small\n",
    "    if np.count_nonzero(submask_union) > 0.3*mask_data[k]['area']:\n",
    "      return True\n",
    "\n",
    "  # print(k, \"Not covered enough - fraction covered:\", np.count_nonzero(submask_union)/mask_data[k]['area'])\n",
    "  return False\n",
    "\n",
    "\n",
    "\n",
    "def filter_masks(mask_data):\n",
    "  # go through masks biggest -> smallest, so we can filter out masks that are mostly in already covered areas (submasks of a larger mask)\n",
    "  sorted_mask_data = sorted(mask_data, key=lambda x: x['area'], reverse=True)\n",
    "  filtered_masks = []\n",
    "\n",
    "  covered_area = np.full(mask_data[0]['segmentation'].shape, False)\n",
    "\n",
    "  for i, mask_object in enumerate(sorted_mask_data):\n",
    "    mask = mask_object['segmentation']\n",
    "\n",
    "    # filter out masks that take up basically the whole image - this will be a zoom level anyways\n",
    "    # this helps us still keep important smaller masks when we filter submasks out\n",
    "    width_fraction = mask_object['bbox'][2] / mask.shape[1]\n",
    "    height_fraction = mask_object['bbox'][3] / mask.shape[0]\n",
    "    area_fraction = mask_object['area'] / mask_object['segmentation'].size\n",
    "    if (width_fraction > 0.9 and height_fraction > 0.9) or area_fraction > 0.5:\n",
    "      continue\n",
    "\n",
    "    # filter out masks that are mostly made up of submasks\n",
    "    if is_made_of_submasks(sorted_mask_data, i):\n",
    "      continue\n",
    "      \n",
    "    # filter out if this is a submask\n",
    "    mask_fraction_already_covered = np.count_nonzero(mask & covered_area) / np.count_nonzero(mask)\n",
    "    if mask_fraction_already_covered > 0.5:\n",
    "      continue\n",
    "\n",
    "    filtered_masks.append(mask_object)\n",
    "    covered_area = covered_area | mask\n",
    "\n",
    "  return filtered_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prominence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_penalty_region_thickness = 0.3  # unit is the image width\n",
    "\n",
    "def get_prominence(mask_object, inverse_depth):\n",
    "  avg_inv_depth = np.mean(inverse_depth[mask_object['segmentation']]).astype('float64')\n",
    "  area_fraction = mask_object['area'] / mask_object['segmentation'].size\n",
    "\n",
    "  bbox = mask_object['bbox']\n",
    "  center_x = bbox[0] + 0.5*bbox[2]\n",
    "  center_y = bbox[1] + 0.5*bbox[3]\n",
    "  img_height, img_width = mask_object['segmentation'].shape\n",
    "  dist_to_side = min(center_x, img_width - center_x, center_y, img_height - center_y)\n",
    "  dist_to_side_factor = min(1, dist_to_side / (border_penalty_region_thickness*img_width))\n",
    "\n",
    "  return (avg_inv_depth**2) * area_fraction * dist_to_side_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def classify(np_image, prompts):\n",
    "    # image is a numpy image, convert to PIL\n",
    "    image = Image.fromarray(np_image)\n",
    "\n",
    "    logging.disable(logging.WARNING)  # get rid of annoying padding message\n",
    "    inputs = clip_processor(text=prompts, images=image, return_tensors=\"pt\", padding=True)  # specifying padding=True seems to help if there are spaces in your input\n",
    "    logging.disable(logging.NOTSET)\n",
    "    \n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "    return prompts[probs.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of utility functions for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    # boxes are XYWH\n",
    "\n",
    "    # extract coordinates and dimensions\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    # calculate intersection area\n",
    "    x_intersection = max(0, min(x1 + w1, x2 + w2) - max(x1, x2))\n",
    "    y_intersection = max(0, min(y1 + h1, y2 + h2) - max(y1, y2))\n",
    "    intersection_area = x_intersection * y_intersection\n",
    "\n",
    "    # calculate union area\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # calculate IoU\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "def deduplicate_boxes(boxs, iou_threshold=0.5):\n",
    "    # deduplicate list of boxs, using intersection over union to identify duplicates\n",
    "    # boxs are XYWH\n",
    "    # keep the larger box when there is a duplicate\n",
    "\n",
    "    deduplicated_boxs = []\n",
    "    \n",
    "    for box in boxs:\n",
    "        is_duplicate = False\n",
    "        for i, existing_box in enumerate(deduplicated_boxs):\n",
    "            iou_value = iou(box, existing_box)\n",
    "            if iou_value >= iou_threshold:\n",
    "                is_duplicate = True\n",
    "                # keep the bigger of the two duplicates\n",
    "                if box[2] * box[3] > existing_box[2] * existing_box[3]:\n",
    "                    deduplicated_boxs[i] = box\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            deduplicated_boxs.append(box)\n",
    "\n",
    "    return deduplicated_boxs\n",
    "\n",
    "\n",
    "def clamp_box_to_image(box, image):\n",
    "    # box is XYWH\n",
    "    x1 = max(0, box[0])\n",
    "    y1 = max(0, box[1])\n",
    "    x2 = min(image.shape[1], box[0] + box[2])\n",
    "    y2 = min(image.shape[0], box[1] + box[3])\n",
    "    return [x1, y1, x2-x1, y2-y1]\n",
    "\n",
    "\n",
    "def expand_to_aspect_ratio(box, aspect_ratio):\n",
    "    # box is XYWH, aspect ratio is width/height\n",
    "\n",
    "    current_ratio = box[2] / box[3]\n",
    "\n",
    "    if current_ratio <= aspect_ratio:\n",
    "        # need to make wider\n",
    "        new_width = aspect_ratio * box[3]\n",
    "        width_to_add = new_width - box[2]\n",
    "        return [box[0] - 0.5*width_to_add, box[1], box[2] + width_to_add, box[3]]\n",
    "    else:\n",
    "        # need to make taller\n",
    "        new_height = box[2] / aspect_ratio\n",
    "        height_to_add = new_height - box[3]\n",
    "        return [box[0], box[1] - 0.5*height_to_add, box[2], box[3] + height_to_add]\n",
    "\n",
    "\n",
    "def scale_and_clamp_box(box, image, scale):\n",
    "    # image is the image to clamp the box inside\n",
    "    center_x = box[0] + 0.5*box[2]\n",
    "    center_y = box[1] + 0.5*box[3]\n",
    "    x1 = np.round(scale*(box[0] - center_x) + center_x)\n",
    "    x2 = np.round(scale*((box[0]+box[2]) - center_x) + center_x)\n",
    "    y1 = np.round(scale*(box[1] - center_y) + center_y)\n",
    "    y2 = np.round(scale*((box[1]+box[3]) - center_y) + center_y)\n",
    "\n",
    "    unclamped = np.array([x1, y1, x2-x1, y2-y1]).astype(int)\n",
    "    return clamp_box_to_image(unclamped, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cropping code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop(box, image, padding_frac=0):\n",
    "    # box is array formatted XYWH\n",
    "    # box is allowed to be outside the image bounds, this function will clamp\n",
    "\n",
    "    x_pad = int(np.round(box[2] * padding_frac))\n",
    "    y_pad = int(np.round(box[3] * padding_frac))\n",
    "\n",
    "    x1 = max(0, box[0] - x_pad)\n",
    "    x2 = min(image.shape[1], box[0] + box[2] + 2*x_pad)\n",
    "    y1 = max(0, box[1] - y_pad)\n",
    "    y2 = min(image.shape[0], box[1] + box[3] + 2*y_pad)\n",
    "    return image[y1:y2, x1:x2]\n",
    "\n",
    "\n",
    "def plot_crops(base_box, image, full_res_image=None, zoom_factor=3, pdf_file_object=None):\n",
    "    # base_box is xywh\n",
    "    # if you provide full_res_image, crops will use it instead\n",
    "    # still need main image so we can interpret bbox coords\n",
    "\n",
    "    image_to_crop = image\n",
    "\n",
    "    # ensure one dimension is not too much longer than the other\n",
    "    max_aspect_ratio = 3\n",
    "    if base_box[2] / base_box[3] > max_aspect_ratio:\n",
    "        base_box = expand_to_aspect_ratio(base_box, max_aspect_ratio)\n",
    "        # print(\"making taller\")\n",
    "    elif base_box[2] / base_box[3] < 1/max_aspect_ratio:\n",
    "        base_box = expand_to_aspect_ratio(base_box, 1/max_aspect_ratio)\n",
    "        # print(\"making wider\")\n",
    "    base_box = clamp_box_to_image(base_box, image)\n",
    "\n",
    "    if full_res_image is not None:\n",
    "        # print(\"Using full_res_image for crops\")\n",
    "        base_box = (np.array(base_box) *\n",
    "                    full_res_image.shape[0] / image.shape[0]).astype(int)\n",
    "        image_to_crop = full_res_image\n",
    "\n",
    "    # do crop of the object\n",
    "    crops = [get_crop(base_box, image_to_crop)]\n",
    "    crop_boxes = [base_box]  # so we can draw the box on the zoomed out images\n",
    "\n",
    "    # when zooming out, do so on a square box containing the mask bbox\n",
    "    # this provides a more standard zoom out in the case the bbox is a weird shape\n",
    "    square_box = expand_to_aspect_ratio(base_box, 1)\n",
    "\n",
    "    for z in [1, 2]:\n",
    "        # stop zooming out if we're nearing the image dimensions\n",
    "        # if the max dimension is almost the whole image, we hit image size\n",
    "        # if the min dimension is an appreciable fraction, the next zoom won't do much\n",
    "        x_frac = crops[-1].shape[1] / image_to_crop.shape[1]\n",
    "        y_frac = crops[-1].shape[0] / image_to_crop.shape[0]\n",
    "        if len(crops) > 0 and (min(x_frac, y_frac) > 0.5 or max(x_frac, y_frac) > 0.8):\n",
    "            break\n",
    "\n",
    "        box = scale_and_clamp_box(square_box, image_to_crop, zoom_factor**z)\n",
    "        crops.append(get_crop(box, image_to_crop))\n",
    "        crop_boxes.append(box)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    for k, c in enumerate(crops):\n",
    "        axis = plt.subplot(1, len(crops), k+1)\n",
    "        axis.imshow(c)\n",
    "        axis.axis(\"off\")\n",
    "\n",
    "        # show bounding box\n",
    "        if k >= 1:\n",
    "            x = base_box[0] - crop_boxes[k][0]\n",
    "            y = base_box[1] - crop_boxes[k][1]\n",
    "            axis.add_patch(plt.Rectangle(\n",
    "                (x, y), base_box[2], base_box[3], edgecolor='red', facecolor=(0, 0, 0, 0), lw=1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if pdf_file_object is not None:\n",
    "        plt.savefig(pdf_file_object, format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext  # for if not saving to pdf\n",
    "\n",
    "def show(image, mask_data=None, title=\"\", figsize=(7,5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    plt.imshow(image)\n",
    "    if mask_data is not None:\n",
    "        show_anns(mask_data)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def process_image(image_path, show_steps=True, pdf_savepath=None):\n",
    "    # load image\n",
    "    image, full_res_image = load_image(image_path)\n",
    "    show(image, title=\"Input Image\")\n",
    "\n",
    "    # find flowers using clipseg\n",
    "    clip_prompts = [\"flower\", \"leaf\", \"sky\", \"rock\", \"dirt\", \"animal\", \"person\", \"human being\"]\n",
    "    flower_bboxs = find_objects(image, clip_prompts, target_prompt=\"flower\", display_results=show_steps, logit_threshold=0.4)  # this threshold seems to work fine\n",
    "\n",
    "    # get segmentation masks\n",
    "    mask_data, unfiltered_mask_data = get_masks(image)\n",
    "    if show_steps:\n",
    "        show(image, unfiltered_mask_data, title=\"Unfiltered Masks\")\n",
    "        show(image, mask_data, title=\"Filtered Masks\")\n",
    "\n",
    "    # get depth\n",
    "    inverse_depth = get_inverse_depth(image)\n",
    "    if show_steps:\n",
    "        show(inverse_depth, title=\"Depth Map\")\n",
    "\n",
    "    # # run segmentation on the depth map?\n",
    "    # depth_rgb = (np.stack((inverse_depth, inverse_depth, inverse_depth), axis=2) * 255).astype(\"uint8\")\n",
    "    # depth_mask_data, unfiltered_depth_mask_data = get_masks(depth_rgb)\n",
    "    # show(image, unfiltered_depth_mask_data, title=\"Unfiltered Masks from Depth Input\")\n",
    "    # show(image, depth_mask_data, \"Filtered Masks from Depth Input\")\n",
    "\n",
    "    # sort masks by prominence\n",
    "    for mask_object in mask_data:\n",
    "        mask_object['prominence'] = get_prominence(mask_object, inverse_depth)\n",
    "    mask_data.sort(key=lambda x: x['prominence'], reverse=True)\n",
    "    if show_steps:\n",
    "        show(image, mask_data[:10], title=\"Top 10 Most Prominent Masks\")\n",
    "\n",
    "    # create list of proposed bboxs\n",
    "    selected_mask_data = mask_data[:10]\n",
    "    bboxs = flower_bboxs + [mask_object[\"bbox\"] for mask_object in selected_mask_data]\n",
    "\n",
    "    # filter out super tiny bboxs\n",
    "    min_bbox_area = 2500\n",
    "    bboxs = list(filter(lambda x: x[2] * x[3] >= min_bbox_area, bboxs))\n",
    "    # filter out duplicate bboxs (ClipSeg flower bboxes might be duplicates of SAM bboxes)\n",
    "    bboxs = deduplicate_boxes(bboxs, iou_threshold=0.25)\n",
    "    # filter out non plant bboxs\n",
    "    plant_bboxs = []\n",
    "    for box in bboxs:\n",
    "        # use full res crop for classification, it works better\n",
    "        full_res_box = (np.array(box) *\n",
    "                    full_res_image.shape[0] / image.shape[0]).astype(int)\n",
    "        crop = get_crop(full_res_box, full_res_image)\n",
    "        crop_class = classify(crop, [\"plant\", \"sky\", \"dirt\", \"human\", \"rock\", \"water\", \"vehicle\"])\n",
    "        if show_steps:\n",
    "            show(crop, title=crop_class)\n",
    "        if crop_class == \"plant\":\n",
    "            plant_bboxs.append(box)\n",
    "    bboxs = plant_bboxs\n",
    "\n",
    "    # generate output visualization\n",
    "    with PdfPages(pdf_savepath) if pdf_savepath else nullcontext() as pdf:\n",
    "        # pdf will equal None if we aren't saving\n",
    "        if pdf_savepath:\n",
    "            plt.figure(figsize=(9,9))\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.imshow(full_res_image)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(pdf, format='pdf')\n",
    "            plt.close()  # don't show this image in the notebook, only for the PDF\n",
    "        for bbox in bboxs:\n",
    "            plot_crops(bbox, image, full_res_image, zoom_factor=3, pdf_file_object=pdf)\n",
    "            # print(\"area\", bbox[2] * bbox[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = R\"G:\\.shortcut-targets-by-id\\1BCCfXZq98f4rFOF8m0AweybL4qQakAUB\\Summer Dive 2022\\Monteverde Imagery - Stephanie\\River\\Dry Flight\\100MEDIA\"\n",
    "# image_basename = \"P0870381.JPG\"  # fern, shrub (close-up)\n",
    "# image_basename = \"P0950389.JPG\"  # fern, shrub, star plant (close-up)\n",
    "# image_basename = \"P1070401.JPG\"  # jungly\n",
    "# image_basename = \"P1360430.JPG\"  # shrubs, colorful leaves\n",
    "# image_basename = \"P1420437.JPG\"  # epiphyte and shrubs\n",
    "# image_basename = \"P0800374.JPG\"  # vine\n",
    "image_basename = \"P1480443.JPG\"  # fruit, and forest floor\n",
    "# image_basename = \"P1540449.JPG\"  # stream surrounded by bushes\n",
    "# image_basename = \"P1550450.JPG\"  # fungus on a log and a few plants\n",
    "# image_basename = \"P2110506.JPG\"  # canopy tree with lots of pale pink fruits\n",
    "# image_basename = \"P2330528.JPG\"  # flowers on bush, and sky\n",
    "\n",
    "\n",
    "\n",
    "# directory_path = R\"C:\\Users\\duker\\Documents\\id_from_image\\durham_test_images\"\n",
    "# image_basename = \"edge_canopy.png\"\n",
    "# image_basename = \"looking_down.png\"\n",
    "# image_basename = \"motion_blur.png\"\n",
    "# image_basename = \"oak_tree.png\"\n",
    "\n",
    "# directory_path = R\"C:\\Users\\duker\\Documents\\id_from_image\\clip_test_images\"\n",
    "# image_basename = \"street.JPG\"\n",
    "\n",
    "\n",
    "image_path = os.path.join(directory_path, image_basename)\n",
    "pdf_savepath = os.path.join(\"output\", f\"{os.path.splitext(image_basename)[0]}.pdf\")\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "process_image(image_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline on Flower Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monteverde_basenames_by_flower_probability = ['P1500445.JPG', 'P1510446.JPG', 'P2270522.JPG', 'P1480443.JPG', 'P2320527.JPG', 'P2340529.JPG', 'P1270421.JPG', 'P1490444.JPG', 'P2280523.JPG', 'P1360430.JPG', 'P2110506.JPG', 'P1520447.JPG', 'P2260521.JPG', 'P2290524.JPG', 'P2330528.JPG', 'P2310526.JPG', 'P2300525.JPG', 'P1910486.JPG', 'P2090504.JPG', 'P1300424.JPG', 'P0640358.JPG', 'P0660360.JPG', 'P1260420.JPG', 'P2240519.JPG', 'P1900485.JPG', 'P1340428.JPG', 'P1950490.JPG', 'P1250419.JPG', 'P1940489.JPG', 'P1990494.JPG', 'P1780473.JPG', 'P2120507.JPG', 'P1310425.JPG', 'P2100505.JPG', 'P1060400.JPG', 'P2190514.JPG', 'P0710365.JPG', 'P1890484.JPG', 'P0780372.JPG', 'P1170411.JPG', 'P1290423.JPG', 'P0650359.JPG', 'P1660461.JPG', 'P0730367.JPG', 'P2210516.JPG', 'P1580453.JPG', 'P1850480.JPG', 'P1420437.JPG', 'P2010496.JPG', 'P1930488.JPG', 'P2200515.JPG', 'P2250520.JPG', 'P2000495.JPG', 'P2220517.JPG', 'P2070502.JPG', 'P1860481.JPG', 'P0890383.JPG', 'P1350429.JPG', 'P1240418.JPG', 'P0770371.JPG', 'P1570452.JPG', 'P1180412.JPG', 'P1280422.JPG', 'P1920487.JPG', 'P2020497.JPG', 'P1230417.JPG', 'P1400435.JPG', 'P2080503.JPG', 'P1410436.JPG', 'P0920386.JPG', 'P1220416.JPG', 'P2230518.JPG', 'P1550450.JPG',\n",
    "#                                               'P0720366.JPG', 'P1870482.JPG', 'P1070401.JPG', 'P1840479.JPG', 'P0840378.JPG', 'P0750369.JPG', 'P1650460.JPG', 'P1190413.JPG', 'P1030397.JPG', 'P1700465.JPG', 'P1880483.JPG', 'P0990393.JPG', 'P0790373.JPG', 'P1630458.JPG', 'P1670462.JPG', 'P2050500.JPG', 'P1640459.JPG', 'P1540449.JPG', 'P0930387.JPG', 'P2160511.JPG', 'P0760370.JPG', 'P1010395.JPG', 'P0800374.JPG', 'P1690464.JPG', 'P0950389.JPG', 'P0850379.JPG', 'P1960491.JPG', 'P0860380.JPG', 'P2150510.JPG', 'P0910385.JPG', 'P0900384.JPG', 'P1980493.JPG', 'P2060501.JPG', 'P1680463.JPG', 'P0820376.JPG', 'P1970492.JPG', 'P1710466.JPG', 'P2170512.JPG', 'P1750470.JPG', 'P1820477.JPG', 'P1810476.JPG', 'P0970391.JPG', 'P1770472.JPG', 'P1560451.JPG', 'P0980392.JPG', 'P0830377.JPG', 'P0810375.JPG', 'P1720467.JPG', 'P1080402.JPG', 'P1730468.JPG', 'P1090403.JPG', 'P1110405.JPG', 'P2180513.JPG', 'P1100404.JPG', 'P1320426.JPG', 'P0870381.JPG', 'P1610456.JPG', 'P1120406.JPG', 'P2040499.JPG', 'P1000394.JPG', 'P1800475.JPG', 'P1530448.JPG', 'P1130407.JPG', 'P1830478.JPG', 'P2140509.JPG', 'P2030498.JPG', 'P1620457.JPG', 'P0960390.JPG', 'P1740469.JPG', 'P2130508.JPG', 'P1760471.JPG', 'P1590454.JPG', 'P1330427.JPG', 'P0680362.JPG', 'P1790474.JPG', 'P0690363.JPG']\n",
    "\n",
    "# os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# directory_path = R\"G:\\.shortcut-targets-by-id\\1BCCfXZq98f4rFOF8m0AweybL4qQakAUB\\Summer Dive 2022\\Monteverde Imagery - Stephanie\\River\\Dry Flight\\100MEDIA\"\n",
    "# for i, image_basename in enumerate(monteverde_basenames_by_flower_probability[:10]):\n",
    "#     image_path = os.path.join(directory_path, image_basename)\n",
    "#     pdf_savepath = os.path.join(\"output\", f\"{i}.pdf\")\n",
    "#     process_image(image_path, pdf_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline on a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# directory_path = R\"G:\\.shortcut-targets-by-id\\1p3LKK2ES2T3ckR3ELLs2AHvp-pvuRZNn\\Summer Dive 2023\\Drone Photos Videos\\Test Day 1\\Ryan_mini\\F1\"\n",
    "# for i, image_basename in enumerate(os.listdir(directory_path)):\n",
    "#     if not image_basename.endswith(\"JPG\"):\n",
    "#         continue\n",
    "#     image_path = os.path.join(directory_path, image_basename)\n",
    "#     pdf_savepath = os.path.join(\"output\", f\"{i}.pdf\")\n",
    "#     process_image(image_path, False, pdf_savepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_id",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
